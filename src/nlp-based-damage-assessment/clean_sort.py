import pandas as pd
# cleaner.py
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
import re

def clean_text(text):
    text = str(text)
    text = re.sub(r'\s+', ' ', text)  # Fazla boÅŸluklarÄ± temizle
    text = re.sub(r'\n|\r', ' ', text)
    text = re.sub(r'[^a-zA-Z0-9ÄŸÃ¼ÅŸÃ¶Ã§Ä°ÄÃœÅÃ–Ã‡.,;:!? ]', '', text)  # TÃ¼rkÃ§e karakter uyumlu temizlik
    return text.strip()

def clean_and_sort_csv(input_path, output_path):
    df = pd.read_csv(input_path)
    print(f"ğŸ“„ YÃ¼klenen veri: {len(df)} satÄ±r")

    # BoÅŸ veya eksik metinleri at
    df = df[df['text'].notna()]
    df['text'] = df['text'].apply(clean_text)

    # Tarihe gÃ¶re sÄ±rala (varsa)
    if 'date' in df.columns:
        df = df.sort_values(by='date', ascending=True)

    # Temiz dosyayÄ± kaydet
    df.to_csv(output_path, index=False, encoding="utf-8-sig")
    print(f"âœ… TemizlenmiÅŸ veri '{output_path}' dosyasÄ±na kaydedildi.")

def run_tfidf_analysis(csv_path, column="text", max_features=20):
    df = pd.read_csv(csv_path)
    corpus = df[column].dropna().tolist()

    vectorizer = TfidfVectorizer(max_features=max_features, stop_words='turkish')
    X = vectorizer.fit_transform(corpus)

    tfidf_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())
    tfidf_sum = tfidf_df.sum().sort_values(ascending=False)

    print("\nğŸ“Š En yÃ¼ksek TF-IDF deÄŸerine sahip kelimeler:")
    print(tfidf_sum.head(20))


def clean_and_sort_csv(input_csv, output_csv):
    df = pd.read_csv(input_csv)

    # Tarih sÃ¼tununu datetime formatÄ±na Ã§evir
    if 'date' in df.columns:
        df['date'] = pd.to_datetime(df['date'], errors='coerce')  # hatalÄ± formatlarÄ± NaT yapar

        # Tarihe gÃ¶re sÄ±ralama (en yeni en Ã¼stte)
        df = df.sort_values(by='date', ascending=False)
    else:
        print("âš ï¸ 'date' sÃ¼tunu bulunamadÄ±, sÄ±ralama yapÄ±lmadÄ±.")

    # Duplicates temizleme
    df = df.drop_duplicates(subset=['url'])
    df = df.drop_duplicates(subset=['text'])

    # Yeni temizlenmiÅŸ CSV olarak kaydet
    #df.to_csv(output_csv, index=False, encoding='utf-8-sig')
    #print(f"âœ… TemizlenmiÅŸ ve sÄ±ralanmÄ±ÅŸ CSV '{output_csv}' olarak kaydedildi.")


import pandas as pd
import re

# 1. Metin Temizleme Fonksiyonu
def clean_text(text):
    if pd.isna(text):
        return ""
    text = str(text)
    text = re.sub(r'\s+', ' ', text)  # fazla boÅŸluklarÄ± kaldÄ±r
    text = re.sub(r'http\S+', '', text)  # linkleri kaldÄ±r
    text = re.sub(r'<.*?>', '', text)  # HTML tagleri kaldÄ±r
    text = re.sub(r'[^a-zA-Z0-9Ã§ÄŸÄ±Ã¶ÅŸÃ¼Ã‡ÄÄ°Ã–ÅÃœ .,!?]', '', text)  # Ã¶zel karakterleri kaldÄ±r
    return text.strip()

# 2. Åehir EÅŸleÅŸtirme Fonksiyonu
def match_city(text, cities):
    matches = [city for city in cities if city.lower() in text.lower()]
    return ", ".join(matches)

# 3. Temizleme, Tarih SÄ±ralama ve Yeni CSVâ€™ye Kaydetme
def clean_and_sort_csv(input_csv, output_csv):
    print(f"ğŸ“¥ Girdi dosyasÄ±: {input_csv}")
    df = pd.read_csv(input_csv)

    # Metin temizliÄŸi
    df['text'] = df['text'].apply(clean_text)

    # Tarih formatÄ± ve sÄ±ralama
    if 'date' in df.columns:
        df['date'] = pd.to_datetime(df['date'], errors='coerce')
        df = df.sort_values(by='date', ascending=False)

    # Duplicates temizleme
    df = df.drop_duplicates(subset=['url'])
    df = df.drop_duplicates(subset=['text'])

    # Åehir eÅŸleme (2023 depreminden etkilenen iller)
    cities = [
        "Adana", "AdÄ±yaman", "DiyarbakÄ±r", "ElazÄ±ÄŸ", "Gaziantep",
        "Hatay", "KahramanmaraÅŸ", "Kilis", "Malatya", "Osmaniye", 
        "ÅanlÄ±urfa"
    ]
    df['cities'] = df['text'].apply(lambda x: match_city(str(x), cities))

    # Yeni dosyaya yaz
    df.to_csv(output_csv, index=False, encoding="utf-8-sig")
    print(f"âœ… TemizlenmiÅŸ ve sÄ±ralanmÄ±ÅŸ veri '{output_csv}' olarak kaydedildi.")

def clean_text(text):
    text = str(text)
    text = re.sub(r'\s+', ' ', text)  # Fazla boÅŸluklarÄ± temizle
    text = re.sub(r'\n|\r', ' ', text)
    text = re.sub(r'[^a-zA-Z0-9ÄŸÃ¼ÅŸÃ¶Ã§Ä°ÄÃœÅÃ–Ã‡.,;:!? ]', '', text)  # TÃ¼rkÃ§e karakter uyumlu temizlik
    return text.strip()

def clean_and_sort_csv(input_path, output_path):
    df = pd.read_csv(input_path)
    print(f"ğŸ“„ YÃ¼klenen veri: {len(df)} satÄ±r")

    # BoÅŸ veya eksik metinleri at
    df = df[df['text'].notna()]
    df['text'] = df['text'].apply(clean_text)

    # Tarihe gÃ¶re sÄ±rala (varsa)
    if 'date' in df.columns:
        df = df.sort_values(by='date', ascending=True)

    # Temiz dosyayÄ± kaydet
    df.to_csv(output_path, index=False, encoding="utf-8-sig")
    print(f"âœ… TemizlenmiÅŸ veri '{output_path}' dosyasÄ±na kaydedildi.")

def run_tfidf_analysis(csv_path, column="text", max_features=20):
    df = pd.read_csv(csv_path)
    corpus = df[column].dropna().tolist()

    # 1ï¸âƒ£ TÃ¼rkÃ§e stop kelimeleri Ã¶nce tanÄ±mla
    turkish_stop_words = [
        "acaba", "ama", "aslÄ±nda", "az", "bazÄ±", "belki", "biri", "birkaÃ§", "birÅŸey", "biz", "bu", "Ã§ok",
        "Ã§Ã¼nkÃ¼", "da", "daha", "de", "defa", "diye", "en", "gibi", "hem", "hep", "hepsi", "her", "hiÃ§",
        "iÃ§in", "ile", "ise", "kez", "ki", "kim", "mÄ±", "mu", "mÃ¼", "nasÄ±l", "ne", "neden", "nerde",
        "nerede", "nereye", "niÃ§in", "niye", "o", "sanki", "ÅŸey", "siz", "ÅŸu", "tÃ¼m", "ve", "veya",
        "ya", "yani"
    ]

    # 2ï¸âƒ£ Vectorizer burada tanÄ±mlanmalÄ±
    vectorizer = TfidfVectorizer(max_features=max_features, stop_words=turkish_stop_words)
    X = vectorizer.fit_transform(corpus)

    tfidf_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())
    tfidf_sum = tfidf_df.sum().sort_values(ascending=False)

    print("\nğŸ“Š En yÃ¼ksek TF-IDF deÄŸerine sahip kelimeler:")
    print(tfidf_sum.head(20))


from wordcloud import WordCloud
import matplotlib.pyplot as plt

def show_wordcloud(csv_path, column="text"):
    df = pd.read_csv(csv_path)
    text = " ".join(df[column].dropna().tolist())
    wc = WordCloud(width=800, height=400, background_color="white", stopwords=set(stopwords.words("turkish"))).generate(text)

    plt.figure(figsize=(12, 6))
    plt.imshow(wc, interpolation="bilinear")
    plt.axis("off")
    plt.title("WordCloud: En SÄ±k GeÃ§en Kelimeler")
    plt.show()



# Direkt Ã§alÄ±ÅŸtÄ±rmak iÃ§in:
if __name__ == "__main__":
    input_csv = "orijinal_code_gazete.csv"
    output_csv = "temizlenmis__sorted_haberler.csv"

    clean_and_sort_csv(input_csv, output_csv)
    run_tfidf_analysis(output_csv)





